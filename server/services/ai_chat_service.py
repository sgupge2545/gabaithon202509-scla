"""
AI ãƒãƒ£ãƒƒãƒˆã‚µãƒ¼ãƒ“ã‚¹
@ludus ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ã«å¯¾ã—ã¦AIãŒè¿”ä¿¡ã™ã‚‹æ©Ÿèƒ½
"""

import logging
import re
from typing import List, Optional, Tuple

from sqlalchemy.orm import Session

from ..database.models import Doc, DocChunk, User
from ..services.doc_service import deserialize_vector
from ..services.embedding import create_single_embedding
from ..services.llm_service import llm

logger = logging.getLogger(__name__)


class AIChatService:
    """AI ãƒãƒ£ãƒƒãƒˆã‚µãƒ¼ãƒ“ã‚¹"""

    @staticmethod
    def ensure_system_user(db: Session) -> None:
        """ã‚·ã‚¹ãƒ†ãƒ ãƒ¦ãƒ¼ã‚¶ãƒ¼ï¼ˆLudusï¼‰ãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèª"""
        try:
            system_user = db.query(User).filter(User.id == "system").first()
            if not system_user:
                system_user = User(
                    id="system",
                    idp_id="system",
                    email="ludus@system.local",
                    name="Ludus",
                    picture_url=None,
                )
                db.add(system_user)
                db.commit()
                logger.info("Created system user (Ludus)")
        except Exception as e:
            logger.error(f"Failed to ensure system user: {e}")

    @staticmethod
    def should_respond_to_message(content: str) -> bool:
        """ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«@ludusãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        # @ludus ã¾ãŸã¯ @Ludus ã¾ãŸã¯ @LUDUS ã«ãƒãƒƒãƒ
        pattern = r"@ludus\b"
        return bool(re.search(pattern, content, re.IGNORECASE))

    @staticmethod
    def extract_user_message(content: str) -> str:
        """@ludusãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ã‚’é™¤ã„ãŸå®Ÿéš›ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å†…å®¹ã‚’æŠ½å‡º"""
        # @ludus ã‚’å‰Šé™¤ã—ã¦ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        pattern = r"@ludus\s*"
        cleaned = re.sub(pattern, "", content, flags=re.IGNORECASE).strip()
        return cleaned

    @staticmethod
    def cosine_similarity(vec1: List[float], vec2: List[float]) -> float:
        """ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—"""
        try:
            import math

            dot_product = sum(a * b for a, b in zip(vec1, vec2))
            magnitude1 = math.sqrt(sum(a * a for a in vec1))
            magnitude2 = math.sqrt(sum(a * a for a in vec2))

            if magnitude1 == 0 or magnitude2 == 0:
                return 0.0

            return dot_product / (magnitude1 * magnitude2)
        except Exception:
            return 0.0

    @staticmethod
    async def search_relevant_chunks(
        db: Session, query_text: str, top_k: int = 5
    ) -> List[Tuple[DocChunk, float]]:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«é–¢é€£ã™ã‚‹ãƒãƒ£ãƒ³ã‚¯ã‚’ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã§å–å¾—"""
        try:
            # ã‚¯ã‚¨ãƒªã‚’embedding
            query_embedding = create_single_embedding(query_text)
            if not query_embedding:
                return []

            # å…¨ã¦ã®ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—
            chunks = db.query(DocChunk).filter(DocChunk.embedding.is_not(None)).all()

            similarities = []
            for chunk in chunks:
                try:
                    # ãƒãƒ£ãƒ³ã‚¯ã®embeddingã‚’ãƒ‡ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º
                    chunk_embedding = deserialize_vector(chunk.embedding)

                    # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—
                    similarity = AIChatService.cosine_similarity(
                        query_embedding, chunk_embedding
                    )
                    similarities.append((chunk, similarity))

                except Exception as e:
                    logger.warning(f"Failed to process chunk {chunk.id}: {e}")
                    continue

            # é¡ä¼¼åº¦ã§ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½top_kã‚’è¿”ã™
            similarities.sort(key=lambda x: x[1], reverse=True)
            return similarities[:top_k]

        except Exception as e:
            logger.error(f"Vector search failed: {e}")
            return []

    @staticmethod
    async def generate_ai_response(
        user_message: str, user_name: str = "ãƒ¦ãƒ¼ã‚¶ãƒ¼", db: Optional[Session] = None
    ) -> Tuple[Optional[str], List[dict]]:
        """AIã®è¿”ä¿¡ã‚’ç”Ÿæˆï¼ˆRAGæ©Ÿèƒ½ä»˜ãï¼‰"""
        referenced_docs_info = []  # å‚è€ƒè³‡æ–™ã®æƒ…å ±ã‚’æ ¼ç´

        try:
            if not llm:
                return "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€ç¾åœ¨AIã‚µãƒ¼ãƒ“ã‚¹ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ ğŸ˜…", []

            # RAG: é–¢é€£ã™ã‚‹è³‡æ–™ã‚’æ¤œç´¢
            relevant_context = ""
            if db:
                try:
                    relevant_chunks = await AIChatService.search_relevant_chunks(
                        db, user_message, top_k=5
                    )

                    if relevant_chunks:
                        context_parts = []
                        referenced_docs = set()  # å‚è€ƒã«ã—ãŸè³‡æ–™ã®ãƒ•ã‚¡ã‚¤ãƒ«åã‚’è¨˜éŒ²

                        for chunk, similarity in relevant_chunks:
                            if similarity > 0.3:  # é¡ä¼¼åº¦ã®é–¾å€¤
                                # ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰é–¢é€£ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—
                                doc = (
                                    db.query(Doc).filter(Doc.id == chunk.doc_id).first()
                                )
                                if doc:
                                    referenced_docs.add(doc.filename)
                                    # å‚è€ƒè³‡æ–™ã®æƒ…å ±ã‚’ä¿å­˜
                                    doc_info = {
                                        "doc_id": doc.id,
                                        "filename": doc.filename,
                                    }
                                    if doc_info not in referenced_docs_info:
                                        referenced_docs_info.append(doc_info)

                                    context_parts.append(
                                        f"[è³‡æ–™: {doc.filename}] {chunk.content[:300]}..."
                                    )

                        if context_parts:
                            relevant_context = "\n\n".join(context_parts)
                            logger.info(
                                f"Found {len(context_parts)} relevant chunks for query: {user_message[:50]}..."
                            )

                except Exception as e:
                    logger.warning(
                        f"RAG search failed, falling back to general response: {e}"
                    )

            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
            if relevant_context:
                referenced_files = list(referenced_docs)
                prompt = f"""ã‚ãªãŸã¯Ludusã¨ã„ã†åå‰ã®AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ãƒãƒ£ãƒƒãƒˆãƒ«ãƒ¼ãƒ ã§å‚åŠ è€…ã¨è‡ªç„¶ãªä¼šè©±ã‚’è¡Œã„ã¾ã™ã€‚

ä»¥ä¸‹ã®é–¢é€£è³‡æ–™ã‚’å‚è€ƒã«ã—ã¦ã€è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ï¼š

{relevant_context}

ä»¥ä¸‹ã®ç‚¹ã«æ³¨æ„ã—ã¦ãã ã•ã„ï¼š
- ä¸Šè¨˜ã®é–¢é€£è³‡æ–™ã®å†…å®¹ã‚’åŸºã«ã€å…·ä½“çš„ã§æ­£ç¢ºãªå›ç­”ã‚’ã—ã¦ãã ã•ã„
- è³‡æ–™ã«è¨˜è¼‰ã•ã‚Œã¦ã„ãªã„å†…å®¹ã«ã¤ã„ã¦ã¯æ¨æ¸¬ã›ãšã€ã€Œè³‡æ–™ã«ã¯è¨˜è¼‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€ã¨ä¼ãˆã¦ãã ã•ã„
- ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ã§è¦ªã—ã¿ã‚„ã™ã„å£èª¿ã§è©±ã—ã¦ãã ã•ã„
- å¿…è¦ã«å¿œã˜ã¦çµµæ–‡å­—ã‚’ä½¿ã£ã¦è¦ªã—ã¿ã‚„ã™ã•ã‚’æ¼”å‡ºã—ã¦ãã ã•ã„
- é•·ã™ãã‚‹å›ç­”ã¯é¿ã‘ã€ç°¡æ½”ã«ã¾ã¨ã‚ã¦ãã ã•ã„
- æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„
- å›ç­”ã®æœ€å¾Œã«ã€Œå‚è€ƒï¼š{', '.join(referenced_files)}ã€ã¨å‚è€ƒã«ã—ãŸè³‡æ–™åã‚’å¿…ãšè¨˜è¼‰ã—ã¦ãã ã•ã„

{user_name}ã•ã‚“ã‹ã‚‰ã®è³ªå•: {user_message}

è¿”ä¿¡:"""
            else:
                prompt = f"""ã‚ãªãŸã¯Ludusã¨ã„ã†åå‰ã®AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ãƒãƒ£ãƒƒãƒˆãƒ«ãƒ¼ãƒ ã§å‚åŠ è€…ã¨è‡ªç„¶ãªä¼šè©±ã‚’è¡Œã„ã¾ã™ã€‚

ä»¥ä¸‹ã®ç‚¹ã«æ³¨æ„ã—ã¦ãã ã•ã„ï¼š
- ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ã§è¦ªã—ã¿ã‚„ã™ã„å£èª¿ã§è©±ã—ã¦ãã ã•ã„
- è³ªå•ã«ã¯ä¸€èˆ¬çš„ãªçŸ¥è­˜ã§ç­”ãˆã¦ãã ã•ã„
- å¿…è¦ã«å¿œã˜ã¦çµµæ–‡å­—ã‚’ä½¿ã£ã¦è¦ªã—ã¿ã‚„ã™ã•ã‚’æ¼”å‡ºã—ã¦ãã ã•ã„
- é•·ã™ãã‚‹å›ç­”ã¯é¿ã‘ã€ç°¡æ½”ã«ã¾ã¨ã‚ã¦ãã ã•ã„
- æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„

{user_name}ã•ã‚“ã‹ã‚‰ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {user_message}

è¿”ä¿¡:"""

            # LLMã‚’ç›´æ¥ä½¿ç”¨ã—ã¦AIå¿œç­”ã‚’ç”Ÿæˆ
            response = await llm.ainvoke(prompt)

            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
            response_text = (
                response.content if hasattr(response, "content") else str(response)
            )

            return (
                response_text.strip() if response_text else None,
                referenced_docs_info,
            )

        except Exception as e:
            logger.error(f"Failed to generate AI response: {e}")
            return "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€ç¾åœ¨è¿”ä¿¡ã§ãã¾ã›ã‚“ ğŸ˜…", []


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
ai_chat_service = AIChatService()
